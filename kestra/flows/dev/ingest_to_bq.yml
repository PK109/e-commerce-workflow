id: ingest_to_bq
namespace: dev
description: Files uploaded to GCS are ingested to BigQuery and archived in separate folder

concurrency:
  behavior: QUEUE
  limit: 1

variables:
  gcs_url_base: "gs://{{kv('GCP_BUCKET_URL')}}/{{kv('GCP_DATASET')}}"
  table_name: "e-commerce-data"
  bq_main_table: "{{ kv('GCP_PROJECT_ID') }}.{{ kv('GCP_DATASET') }}.{{ vars.table_name }}"
  file_name: "{{taskrun.value | split('/') | last | split('.csv') | first }}"
  file_url: "gs://{{ kv('GCP_BUCKET_URL') }}/{{ taskrun.value }}"
  bq_staging_table: "{{ kv('GCP_PROJECT_ID') }}.{{ kv('GCP_DATASET') }}.stg_{{ vars.table_name }}"
  bq_external_table: "{{ kv('GCP_PROJECT_ID') }}.{{ kv('GCP_DATASET') }}.ext_{{ vars.table_name }}_{{ vars.file_name }}"
  dbt_command: "dbt build --vars {is_test_run: false}"

tasks:
  - id: each
    type: io.kestra.plugin.core.flow.ForEach
    values: "{{ trigger.blobs | jq('.[].name') }}"
    description: "For each name of the file found in the folder."
    tasks:

      - id: log_values
        type: io.kestra.plugin.core.log.Log
        message: "Working on file: {{ render(vars.file_name) }}"

      - id: vars
        type: io.kestra.plugin.core.output.OutputValues
        values:
          file: "{{ taskrun.value }}"

      - id: bq_main_table_create
        type: io.kestra.plugin.gcp.bigquery.Query
        sql: |
          CREATE TABLE IF NOT EXISTS `{{ render(vars.bq_main_table) }}`
          (
            unique_row_id BYTES OPTIONS (description = 'A unique identifier for the record, generated by hashing key event attributes.'),
            event_time TIMESTAMP OPTIONS (description = 'Time when event happened at (in UTC).'),
            event_type STRING	OPTIONS (description = 'One of [view, cart, remove_from_cart, purchase]'),
            product_id INTEGER OPTIONS (description = 'ID of a product.'),
            category_id INTEGER OPTIONS (description = 'Products category ID'),
            category_code STRING OPTIONS (description = 'Products category taxonomy (code name) if it was possible to make it. Usually present for meaningful categories and skipped for different kinds of accessories.'),
            brand STRING	OPTIONS (description = 'Downcased string of brand name. Can be missed.'),
            price FLOAT64	OPTIONS (description = 'Float price of a product. Present.'),
            user_id INTEGER	OPTIONS (description = 'Permanent user ID.'),
            user_session STRING	OPTIONS (description = 'Temporary users session ID. Same for each users session. Is changed every time user come back to online store from a long pause.')
          )
          PARTITION BY DATE(event_time);

      - id: bq_external_table_create
        type: io.kestra.plugin.gcp.bigquery.Query
        sql: |
          CREATE OR REPLACE EXTERNAL TABLE `{{ render(vars.bq_external_table) }}`
          (
            event_time TIMESTAMP OPTIONS (description = 'Time when event happened at (in UTC).'),
            event_type STRING	OPTIONS (description = 'One of [view, cart, remove_from_cart, purchase]'),
            product_id INTEGER OPTIONS (description = 'ID of a product.'),
            category_id INTEGER OPTIONS (description = 'Products category ID'),
            category_code STRING OPTIONS (description = 'Products category taxonomy (code name) if it was possible to make it. Usually present for meaningful categories and skipped for different kinds of accessories.'),
            brand STRING	OPTIONS (description = 'Downcased string of brand name. Can be missed.'),
            price FLOAT64	OPTIONS (description = 'Float price of a product. Present.'),
            user_id INTEGER	OPTIONS (description = 'Permanent user ID.'),
            user_session STRING	OPTIONS (description = 'Temporary users session ID. Same for each users session. Is changed every time user come back to online store from a long pause.')
          )
          OPTIONS 
          (
          format = 'CSV',

          uris = ['{{ render(vars.file_url) }}'],
          skip_leading_rows = 1
          );

      - id: bq_staging_table_ingest
        type: io.kestra.plugin.gcp.bigquery.Query
        sql: |
          CREATE OR REPLACE TABLE `{{ render(vars.bq_staging_table) }}` 
          AS
              SELECT
                MD5(CONCAT(
                  COALESCE(CAST(event_time AS STRING), ""),
                  COALESCE(CAST(user_session AS STRING), "")
                )) AS unique_row_id,
                *
              FROM `{{ render(vars.bq_external_table) }}`

      - id: bq_main_table_merge
        type: io.kestra.plugin.gcp.bigquery.Query
        sql: |
          MERGE INTO `{{ render(vars.bq_main_table) }}` T
          USING `{{ render(vars.bq_staging_table) }}` S
          ON T.unique_row_id = S.unique_row_id
          WHEN NOT MATCHED THEN
            INSERT (unique_row_id, event_time, event_type, product_id, category_id, category_code, brand, price, user_id, user_session)
            VALUES (S.unique_row_id, S.event_time, S.event_type, S.product_id, S.category_id, S.category_code, S.brand, S.price, S.user_id, S.user_session);

      - id: drop_ext_table
        type: io.kestra.plugin.gcp.bigquery.DeleteTable
        dataset: "{{ kv('GCP_DATASET') }}"
        table: "ext_{{ render(vars.table_name) }}_{{ render(vars.file_name) }}"

      - id: move_to_archive
        type: io.kestra.plugin.gcp.gcs.Copy
        delete: true
        from: gs://{{kv('GCP_BUCKET_URL')}}/{{taskrun.value}}
        to: gs://{{kv('GCP_BUCKET_URL')}}/{{ kv('GCP_DATASET') }}/partitioned/archive/{{ render(vars.file_name) }}.csv

  - id: purge_files
    type: io.kestra.plugin.core.storage.PurgeCurrentExecutionFiles

  - id: dbt-build
    type: io.kestra.plugin.dbt.cli.DbtCLI
    env:
      DBT_DATABASE: "{{kv('GCP_PROJECT_ID')}}"
      DBT_SCHEMA: "{{kv('GCP_DATASET')}}"
    namespaceFiles:
      enabled: true
    containerImage: ghcr.io/kestra-io/dbt-bigquery:latest
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    inputFiles:
      sa.json: "{{kv('GCP_CREDENTIALS')}}"
    commands:
      - dbt deps
      - "{{vars.dbt_command}}"
    storeManifest:
      key: manifest.json
      namespace: "{{ flow.namespace }}"
    profiles: |
      default:
        outputs:
          dev:
            type: bigquery
            dataset: "{{kv('GCP_DATASET')}}"
            project: "{{kv('GCP_PROJECT_ID')}}"
            location: "{{kv('GCP_LOCATION')}}"
            keyfile: sa.json
            method: service-account
            priority: interactive
            threads: 16
            timeout_seconds: 300
            fixed_retries: 1
        target: dev

triggers:
  - id: watch
    type: io.kestra.plugin.gcp.gcs.Trigger
    action: NONE
    from: "{{vars.gcs_url_base}}/partitioned/"
    interval: PT10M
    regExp: .*.csv

pluginDefaults:
  - type: io.kestra.plugin.gcp
    values:
      serviceAccount: "{{kv('GCP_CREDENTIALS')}}"
      projectId: "{{kv('GCP_PROJECT_ID')}}"
      location: "{{kv('GCP_LOCATION')}}"
      bucket: "{{kv('GCP_BUCKET_URL')}}"
