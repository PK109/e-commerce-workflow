id: dlt_dataset_import
namespace: e_commerce

inputs:

  - id: month
    type: SELECT
    displayName: Select period of time
    values: [ "2019-Oct", "2019-Nov", "2019-Dec", "2020-Jan", "2020-Feb", "2020-Mar", "2020-Apr" ]
    defaults: "2019-Oct"

variables:
  file: "{{inputs.month}}.csv.gz"
  gcs_dataset: "{{kv('GCP_DATASET')}}"
  url_base: "https://data.rees46.com/datasets/marketplace"
  # adjust below parameter to fit data into memory
  download_chunk_size_MB: 100 #size of cached data from server in MB
  pandas_chunk_size: 500_000 #size of parsed rows in pandas

tasks:
  - id: dlt_pipeline
    type: io.kestra.plugin.scripts.python.Script
    beforeCommands:
      - pip install dlt[gs] pandas pyarrow
    containerImage: python:3.11
    env:
      DESTINATION__CREDENTIALS: "{{ kv('GCP_CREDENTIALS') }}"
      BUCKET_URL: "gs://{{kv('GCP_BUCKET_NAME')}}"
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    warningOnStdErr: false
    script: |
      import os
      import dlt
      import requests
      import pandas as pd
      from dlt.destinations import filesystem

      dtype_dict = {
          "event_type": "category",
          "product_id": "int32",
          "category_id": "int64",
          "category_code": "category",
          "brand": "category",
          "price": "float32",
          "user_id": "int32",
          "user_session": "string"
      }

      # Define a dlt source to download and process Parquet files as resources
      @dlt.source(name="e_commerce")
      def download_parquet():
          file_name = "{{ render(vars.file) }}"
          url = f"{{ vars.url_base }}/{file_name}"

          # Download the file
          response = requests.get(url, stream=True)
          with open(file_name, 'wb') as f:
              for chunk in response.iter_content(chunk_size= {{ vars.download_chunk_size_MB }} * 1024 * 1024):
                  f.write(chunk)
          # Read and process file in chunks
          chunk_size = {{ vars.pandas_chunk_size }}
          for i, chunk in enumerate(pd.read_csv(file_name, dtype=dtype_dict, parse_dates=["event_time"], chunksize=chunk_size)):
              
              print(f"Processing chunk {i:03d}:\t Row count:\t{len(chunk)}\t Size of chunk: {chunk.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
              yield dlt.resource(chunk, name=f"{{inputs.month}}_chunk_{i:03d}", write_disposition='replace')

      # Initialize the pipeline
      pipeline = dlt.pipeline(
          pipeline_name="sales_data",
          destination=dlt.destinations.filesystem(
              layout="{schema_name}/{table_name}.{ext}"
          ),
          dataset_name="{{vars.gcs_dataset}}"
      )

      load_info = pipeline.run(
          download_parquet(),
          loader_file_format="parquet"
          )

      # Print the results
      print(load_info)

pluginDefaults:
  - type: io.kestra.plugin.gcp
    values:
      serviceAccount: "{{kv('GCP_CREDS')}}"
      projectId: "{{kv('GCP_PROJECT_ID')}}"
      location: "{{kv('GCP_LOCATION')}}"
      bucket: "{{kv('GCP_BUCKET_NAME')}}"
